#数据挖掘部分代码
import requests
from bs4 import BeautifulSoup
import csv
import pandas as pd
import jieba

#爬虫部分
#CSS相关知识：类用点，id用#，标签直接写。类中不可有空格，如 .intern-wrap.intern-item
word_filter = ['职位','描述','城市','北京','上海','广州','深圳','工作','内容','日常','其他','事务','并非','包含','介意','看清','要求','以上','岗位职责',
               '能力','能够','公司','正式','机会','时间','之后','本科','以上学历','每周','出勤','至少','以上','获得','薪酬','餐补','保证','愿意','协助']
with open('job_information.csv','w',newline='',encoding='utf-8') as fb:
    writer = csv.writer(fb)
    writer.writerow(('enterprise','job','salary','jd_keywords'))
    count = 0
    for i in range(1,7):
        count += 1
        if count %2 == 0:
            print(count)
        url = f'https://www.shixiseng.com/interns?page={i}&keyword=%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90&type=intern&area=&months=&days=&degree=&official=&enterprise=&salary=-0&publishTime=&sortType=&city=%E5%8C%97%E4%BA%AC&internExtend='
        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'}
        req = requests.get(url,headers = headers)
        req.encoding = 'utf-8'
        html = req.text
        soup = BeautifulSoup(html,'lxml')
        divs = soup.select('.intern-wrap.intern-item')
        for div in divs:
            job = div.select_one('.f-l.intern-detail__job p a').text
            enterprise = div.select_one('.f-r.intern-detail__company p a')['title']
            salary = div.select_one('.f-l.intern-detail__job .day.font').text.encode('utf-8')
            innerurl = div.select_one('.f-l.intern-detail__job p a')['href']
            innerurl_req = requests.get(innerurl,headers = headers)
            innerurl_html = innerurl_req.text
            innerurl_html_soup = BeautifulSoup(innerurl_html,'lxml')
            jd_words = innerurl_html_soup.select_one('div.content_left .job_detail').text
            jd_seg_list = list(jieba.cut(jd_words))
            jd_keywords = pd.Series(jd_seg_list)
            jd_keywords = jd_keywords[jd_keywords.str.len()>1]
            jd_keywords = jd_keywords[~jd_keywords.str.contains('|'.join(word_filter))]
            jd_keywords = jd_keywords.value_counts().sort_values(ascending=False)[:15]
            jd_keywords = jd_keywords.index
            salary = salary.replace(b'\xef\x92\xb2',b'0')
            salary = salary.replace(b'\xef\x96\xab',b'1')
            salary = salary.replace(b'\xee\xae\xb9',b'2')
            salary = salary.replace(b'\xee\x89\x91',b'3')
            salary = salary.replace(b'\xee\x80\x83',b'4')
            salary = salary.replace(b'\xee\x88\x9e',b'5')
            salary = salary.replace(b'\xee\xa7\x9b',b'8')
            salary = salary.decode()
            writer.writerow((enterprise,job,salary,jd_keywords))
